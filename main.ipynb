{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import roypy\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "import queue\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "\n",
    "import tracking\n",
    "import touched_detection\n",
    "from PIL import Image, ImageDraw\n",
    "from sample_camera_info import print_camera_info\n",
    "from roypy_sample_utils import CameraOpener, add_camera_opener_options\n",
    "\n",
    "# 調整np.array輸出格式\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(roypy.IDepthDataListener):\n",
    "\n",
    "    def __init__(self, z_queue, gray_queue, points3D_queue, ConfidenceIndex_queue, Confidence_queue, undistortImage=False):\n",
    "        super(MyListener, self).__init__()\n",
    "        self.z_queue = z_queue\n",
    "        self.gray_queue = gray_queue\n",
    "        self.points3D_queue = points3D_queue\n",
    "        self.ConfidenceIndex_queue = ConfidenceIndex_queue\n",
    "        self.Confidence_queue = Confidence_queue\n",
    "        self.undistortImage = undistortImage\n",
    "        self.cameraMatrix = None\n",
    "        self.distortionCoefficients = None\n",
    "\n",
    "    def onNewData(self, data):\n",
    "        zvalues = []\n",
    "        xvalues = []\n",
    "        yvalues = []\n",
    "        grayvalues = []\n",
    "        points3D = []\n",
    "        ConfidenceIndexvalues = [] # whether the pixel measured a valid 3D value\n",
    "        Confidencevalue = []\n",
    "        for i in range(data.getNumPoints()):\n",
    "            zvalues.append(data.getZ(i))\n",
    "            xvalues.append(data.getX(i))\n",
    "            yvalues.append(data.getY(i))\n",
    "            grayvalues.append(data.getGrayValue(i))\n",
    "            \n",
    "            Confidencevalue.append(data.getDepthConfidence(i))\n",
    "            if data.getDepthConfidence(i) > 0:\n",
    "                ConfidenceIndexvalues.append(i)\n",
    "            \n",
    "        zarray = np.asarray(zvalues)\n",
    "        z = zarray.reshape (-1, data.width)        \n",
    "        self.z_queue.put(z)\n",
    "        \n",
    "        xarray = np.asarray(xvalues)\n",
    "        x = xarray.reshape (-1, data.width)\n",
    "        \n",
    "        yarray = np.asarray(yvalues)\n",
    "        y = yarray.reshape (-1, data.width)\n",
    "        \n",
    "        points3D = np.dstack((x,y,z))\n",
    "        self.points3D_queue.put(points3D)\n",
    "        \n",
    "        \n",
    "        grayarray = np.asarray(grayvalues)\n",
    "        q = grayarray.reshape (-1, data.width)        \n",
    "        self.gray_queue.put(q)\n",
    "        \n",
    "        Confidencearray = np.asarray(Confidencevalue)\n",
    "        Confidence = Confidencearray.reshape (-1, data.width)\n",
    "        self.Confidence_queue.put(Confidence)\n",
    "        #ConfidenceIndex_queue\n",
    "        ConfidenceIndex = np.asarray(ConfidenceIndexvalues)\n",
    "        self.ConfidenceIndex_queue.put(ConfidenceIndex)\n",
    "\n",
    "    def paint(self, data, name, isGray = False, cv2_show = False):\n",
    "        \"\"\"\n",
    "        Called in the main thread, with data containing one of the items that was added to the queue in onNewData\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if cv2_show == True: \n",
    "            cv2.imshow(name, data)\n",
    "        else:\n",
    "            # create a figure and show the raw data\n",
    "            plt.figure(1)\n",
    "            if isGray == False:\n",
    "                plt.imshow(data),plt.title(name),plt.axis('off')\n",
    "            else: \n",
    "                plt.imshow(data,plt.cm.gray),plt.title(name),plt.axis('off')\n",
    "\n",
    "            plt.show(block=False)\n",
    "            plt.draw()\n",
    "\n",
    "            # this pause is needed to ensure the drawing for some backends\n",
    "            plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracker Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event(z_queue, gray_queue, points3D_queue, ConfidenceIndex_queue, Confidence_queue, painter=None, seconds=150):\n",
    "    First = True\n",
    "    timeout = .5\n",
    "    wait_data_flag = False\n",
    "    capture_frame = 5\n",
    "    frame = 0\n",
    "    t_end = time.time() + seconds # create a loop that will rsecondsthe given amount of time\n",
    "    start_time = time.time()\n",
    "    receive_time = 1\n",
    "    while time.time() < t_end:\n",
    "        # Press Q on keyboard to exit \n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "        try:\n",
    "            # try to retrieve an item from the queue\n",
    "            # this will block until an item can be retrieved\n",
    "            # or the timeout of 1 second is hit\n",
    "            zImage = z_queue.get(wait_data_flag, timeout) if z_queue else None\n",
    "            grayImage_int32 = gray_queue.get(wait_data_flag, timeout) if gray_queue else None\n",
    "            points3D = points3D_queue.get(wait_data_flag, timeout) if points3D_queue else None\n",
    "            ConfidenceIndex = ConfidenceIndex_queue.get(wait_data_flag, timeout) if ConfidenceIndex_queue else None\n",
    "            Confidence = Confidence_queue.get(wait_data_flag, timeout) if Confidence_queue else None\n",
    "        except queue.Empty:\n",
    "            # this will be thrown when the timeout is hit\n",
    "            continue\n",
    "        else:\n",
    "            frame = frame + 1\n",
    "            if (frame > capture_frame):\n",
    "                # estimated time\n",
    "                receive_time = (time.time() - start_time)\n",
    "                frame = 0\n",
    "                start_time = time.time()\n",
    "            if(receive_time != 0):\n",
    "                size = zImage.shape\n",
    "                cv2.putText(img=zImage, text=('fps=%.1f'%(5/receive_time)), org=(0, size[1]-75), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.25, color=(255,255,255))\n",
    "            cv2.imshow('zImage', zImage)\n",
    "            \n",
    "            if First == True:\n",
    "                #RANSAM to get surface plane\n",
    "                surface_plane, depthImg, plane_mask = RANSAM(points3D, ConfidenceIndex, ransac_iteration = 500, inliner_threshold = 0.01)\n",
    "                First = False\n",
    "                print(\"firstly RANSAM\")\n",
    "            else:\n",
    "                #\n",
    "                s_time = time.time()\n",
    "                depthImg = get_depth_map(points3D, surface_plane)\n",
    "                print('depth: %.4f'%(time.time() - s_time))\n",
    "                \n",
    "                #Canny edge map(infrared image) + threshold based edge map(depth image)\n",
    "                s_time = time.time()\n",
    "                Cannyedges, Threshold_based_edge, Edge_map, grayImage = get_edge_map(grayImage_int32, depthImg)\n",
    "                print('get edge map: %.4f s'%(time.time() - s_time))\n",
    "                \n",
    "                #Get hight region by Hight and record its position\n",
    "                s_time = time.time()\n",
    "                High_region_Image, high_region_list = get_high_region(depthImg)\n",
    "                print('get hight region: %.4f s'%(time.time() - s_time))\n",
    "                \n",
    "                #Get Hand mask by Flood fill from high region position with Edge map\n",
    "                s_time = time.time()\n",
    "                Hand_mask_Image = get_Hand_mask(Edge_map, high_region_list, High_region_Image)\n",
    "                print('get hand mask: %.4f s'%(time.time() - s_time))\n",
    "                \n",
    "                #\n",
    "                s_time = time.time()\n",
    "                cnt, contours_image, hand_center, fingertips = find_fingertip(Hand_mask_Image)\n",
    "                print('find fingertip: %.4f s'%(time.time() - s_time))\n",
    "                \n",
    "                # tracking user hand and fingertips\n",
    "                s_time = time.time()\n",
    "                hands_hand, fingertips_pos, tracking_iamge = tracking.hand_tracking(hand_center, fingertips, points3D, contours_image)\n",
    "                print('hand tracking: %.4f s'%(time.time() - s_time))\n",
    "                \n",
    "                # detecte fingers whenther touching the surface\n",
    "                s_time = time.time()\n",
    "                touched_flag, touching_detection_image = touched_detection.touching_detection(fingertips_pos, Hand_mask_Image, depthImg, contours_image)\n",
    "                print('touched detection: %.4f s'%(time.time() - s_time))\n",
    "                \n",
    "                #Show original image and result image\n",
    "#                 painter.paint(grayImage_int32,'Gray Image',True)\n",
    "#                 painter.paint(depthImg,'Depth')\n",
    "#                 painter.paint(Cannyedges,'Canny Edges',True)\n",
    "#                 painter.paint(Threshold_based_edge,'Threshold_based Edge',True)\n",
    "#                 painter.paint(Edge_map,'Edge map',True)\n",
    "#                 painter.paint(High_region_Image,'High region Image',True)\n",
    "#                 painter.paint(Hand_mask_Image,'Hand mask Image')\n",
    "#                 painter.paint(contours_image,'Contours Image',True)\n",
    "                painter.paint(tracking_iamge,'tracking Image',False, True)\n",
    "                painter.paint(touching_detection_image,'touched Image', False, True)\n",
    "            \n",
    "#             #Store Image\n",
    "#             IsStore = False\n",
    "#             if IsStore == True:\n",
    "#                 matplotlib.image.imsave('zImage.png', zImage)\n",
    "#                 matplotlib.image.imsave('grayImage_int32.png', grayImage_int32)\n",
    "#                 matplotlib.image.imsave('points3D.png', points3D)\n",
    "#                 matplotlib.image.imsave('Confidence.png', Confidence)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dis_pt2plane(pts, a, b, c, d):\n",
    "    \"\"\"\n",
    "    Compute the distance from points to the plane\n",
    "    \"\"\"\n",
    "    normal = math.sqrt(a*a+b*b+c*c)\n",
    "    if normal == 0:\n",
    "        normal = 1\n",
    "    \n",
    "    v = np.array([a,b,c])\n",
    "    dis = abs(np.dot(pts,v.T)+d)/normal\n",
    "    return dis\n",
    "\n",
    "def get_Plane(sampts):\n",
    "    \"\"\"\n",
    "    Compute the equation of the plane\n",
    "    \"\"\"\n",
    "    p1 = sampts[0]\n",
    "    p2 = sampts[1]\n",
    "    p3 = sampts[2]\n",
    "    \n",
    "    a = ( (p2[1]-p1[1])*(p3[2]-p1[2])-(p2[2]-p1[2])*(p3[1]-p1[1]) )\n",
    "    b = ( (p2[2]-p1[2])*(p3[0]-p1[0])-(p2[0]-p1[0])*(p3[2]-p1[2]) )\n",
    "    c = ( (p2[0]-p1[0])*(p3[1]-p1[1])-(p2[1]-p1[1])*(p3[0]-p1[0]) )\n",
    "    d = ( 0-(a*p1[0]+b*p1[1]+c*p1[2]) )\n",
    "    \n",
    "    return a,b,c,d\n",
    "\n",
    "def Random3points(points3D, ConfidenceIndex):\n",
    "    \"\"\"\n",
    "    Random choose 3 Confidence points\n",
    "    \"\"\"\n",
    "    sample_number = 3\n",
    "    sample_point_index = random.sample(range(ConfidenceIndex.shape[0]), sample_number)\n",
    "    sample_points = np.zeros((sample_number,3))\n",
    "    for i in range(sample_number):\n",
    "        Confidence_point_index = sample_point_index[i]\n",
    "        index = ConfidenceIndex[Confidence_point_index]\n",
    "        y = index // points3D.shape[1]\n",
    "        x = index % points3D.shape[1]\n",
    "        sample_points[i] = points3D[y][x]\n",
    "    return sample_points\n",
    "\n",
    "# def Random3points(points3D):\n",
    "#     sample_number = 3\n",
    "#     sample_point_index = random.sample(range(points3D.shape[0]*points3D.shape[1]), sample_number)\n",
    "#     sample_points = np.zeros((sample_number,3))\n",
    "#     for i in range(sample_number):\n",
    "#         index = sample_point_index[i]\n",
    "#         y = index // points3D.shape[1]\n",
    "#         x = index % points3D.shape[1]\n",
    "#         sample_points[i] = points3D[y][x]\n",
    "#     return sample_points\n",
    "\n",
    "def get_inliner_num(points3D,a,b,c,d,inliner_threshold):\n",
    "    \"\"\"\n",
    "    Compute the liner points which distance to plane < threshold\n",
    "    Also get distance from points to the plane (new Depth Image which re-project depth pixels in surface plane)\n",
    "    \"\"\"\n",
    "    inliner_num = 0\n",
    "    \n",
    "    dist = Dis_pt2plane(points3D,a,b,c,d)\n",
    "    inliner_mask = dist < inliner_threshold\n",
    "    inliner_num = np.sum(inliner_mask)\n",
    "    return inliner_num, inliner_mask, dist\n",
    "\n",
    "def RANSAM(points3D, ConfidenceIndex, ransac_iteration = 1000, inliner_threshold = 0.01):\n",
    "    best_inlinernum = -1\n",
    "    best_inlinernum = 0\n",
    "    best_plane = np.zeros((1,4))\n",
    "    best_depthImage = np.zeros((points3D.shape[0],points3D.shape[1]))\n",
    "    best_plane_mask = np.zeros((points3D.shape[0],points3D.shape[1]))\n",
    "#     best_sampts = np.zeros((3,3))\n",
    "    \n",
    "#     print(points3D.shape,points3D[80:90,110])\n",
    "    for i in range(ransac_iteration):\n",
    "        sampts = Random3points(points3D, ConfidenceIndex)\n",
    "        a,b,c,d = get_Plane(sampts)\n",
    "        \n",
    "        inliner_num, inliner_mask, depthImage = get_inliner_num(points3D,a,b,c,d,inliner_threshold)\n",
    "        if(inliner_num > best_inlinernum):\n",
    "            best_inlinernum = inliner_num\n",
    "            best_plane = np.array([a,b,c,d])\n",
    "            best_plane_mask = inliner_mask\n",
    "            best_depthImage = depthImage\n",
    "#             best_sampts = sampts\n",
    "            \n",
    "    print(\"Inliner Number\\n\", best_inlinernum)\n",
    "    print(\"Inliner plane\\n\", best_plane)\n",
    "    return best_plane, best_depthImage, best_plane_mask\n",
    "\n",
    "# ptset = np.array(([0, 0, 0],\n",
    "#             [1, 2, 0],\n",
    "#             [2, 2, 0]))\n",
    "# a,b,c,d = get_Plane(ptset)\n",
    "# pts = np.zeros((2,2,3))\n",
    "# pts[0,0] = np.array([0, 0, 1])\n",
    "# pts[0,1] = np.array([0, 0, 2])\n",
    "# pts[1,0] = np.array([0, 0, 3])\n",
    "# pts[1,1] = np.array([0, 0, 4])\n",
    "# z = Dis_pt2plane(pts,a,b,c,d)\n",
    "# print(z,z.shape)\n",
    "# get_inliner_num(pts,a,b,c,d,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_map(points3D,plane):\n",
    "    \"\"\"\n",
    "    Compute the liner points which distance to plane < threshold\n",
    "    Also get distance from points to the plane (new Depth Image which re-project depth pixels in surface plane)\n",
    "    \"\"\"\n",
    "    dist = Dis_pt2plane(points3D,plane[0],plane[1],plane[2],plane[3])\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_map(grayImage,depthImage):\n",
    "    \"\"\"\n",
    "    Canny Edge map\n",
    "    turn grayImg from int32 to int8\n",
    "    blur the grayImg then do Canny Edge\n",
    "    \"\"\"\n",
    "    low_threshold = 1\n",
    "    high_threshold = 8\n",
    "    grayimg_int8 = cv2.convertScaleAbs(grayImage, alpha=(255.0/65535.0))\n",
    "    \n",
    "    kernel_size = 3\n",
    "    blur_gray = cv2.GaussianBlur(grayimg_int8,(kernel_size, kernel_size), 0)\n",
    "    Cannyedges = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "    \n",
    "    \"\"\"\n",
    "    Threshold based Edge map\n",
    "    if depth between the pixel and its nearby pixels > near_depth_threshold, then labeled it\n",
    "    \"\"\"\n",
    "    s_time = time.time()\n",
    "    near_depth_threshold = 0.05\n",
    "    Threshold_based_edge = np.zeros((depthImage.shape[0],depthImage.shape[1]))\n",
    "    for y in range(1,depthImage.shape[0]-1):\n",
    "        for x in range(1,depthImage.shape[1]-1):\n",
    "            center = depthImage[y, x]\n",
    "            if(abs(center - depthImage[y-1,x-1]) > near_depth_threshold):\n",
    "                Threshold_based_edge[y,x] = True\n",
    "                continue\n",
    "            if(abs(center - depthImage[y-1,x]) > near_depth_threshold):\n",
    "                Threshold_based_edge[y,x] = True\n",
    "                continue\n",
    "            if(abs(center - depthImage[y-1,x+1]) > near_depth_threshold):\n",
    "                Threshold_based_edge[y,x] = True\n",
    "                continue\n",
    "            if(abs(center - depthImage[y,x-1]) > near_depth_threshold):\n",
    "                Threshold_based_edge[y,x] = True\n",
    "                continue\n",
    "            if(abs(center - depthImage[y,x+1]) > near_depth_threshold):\n",
    "                Threshold_based_edge[y,x] = True\n",
    "                continue\n",
    "            if(abs(center - depthImage[y+1,x-1]) > near_depth_threshold):\n",
    "                Threshold_based_edge[y,x] = True\n",
    "                continue\n",
    "            if(abs(center - depthImage[y+1,x]) > near_depth_threshold):\n",
    "                Threshold_based_edge[y,x] = True\n",
    "                continue\n",
    "            if(abs(center - depthImage[y+1,x+1]) > near_depth_threshold):\n",
    "                Threshold_based_edge[y,x] = True\n",
    "                continue\n",
    "    print('*get threshold edge: %.4f s'%(time.time()-s_time))\n",
    "    \"\"\"\n",
    "    Merge Canny Edge map and Threshold based Edge map\n",
    "    \"\"\"\n",
    "    Edge_map = np.logical_or(Cannyedges,Threshold_based_edge)\n",
    "    \n",
    "    return Cannyedges,Threshold_based_edge, Cannyedges, blur_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find High Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_region(depthImage):\n",
    "    \"\"\"\n",
    "    Define plane edge\n",
    "    High region : > 0.04 \n",
    "    ==> Flood fill mask : if pixels value > 0.04, then its value of mask image = 0 \n",
    "    ==> mask = depthImage < 0.04\n",
    "    \n",
    "    Next, reject the region which dose not connet with plane edge and record its position\n",
    "    ==>Just do Flood fill around the plane edge\n",
    "    \"\"\"\n",
    "    h, w = depthImage.shape[:2]\n",
    "    high_region_mask = np.ones((h+2,w+2), np.uint8)\n",
    "    high_region_mask[1:h+1,1:w+1] = depthImage < 0.04 # > 0.04 False == 0, Flood fill will fill pixels with 0\n",
    "    resultImg = np.zeros((h,w), np.uint8)\n",
    "    \n",
    "    #define plane edge\n",
    "    plane_edge = 25\n",
    "    \n",
    "    x1 = plane_edge\n",
    "    x2 = w - plane_edge\n",
    "    y1 = plane_edge\n",
    "    y2 = h - plane_edge\n",
    "    \n",
    "    high_list = []\n",
    "    \n",
    "    for y in range(plane_edge, y2):\n",
    "        if high_region_mask[y+1,x1+1] == 0 and resultImg[y,x1] != True:\n",
    "            cv2.floodFill(resultImg, high_region_mask, (x1, y),True, cv2.FLOODFILL_MASK_ONLY)\n",
    "            high_list.append((x1,y))\n",
    "            \n",
    "        if high_region_mask[y+1,x2+1] == 0 and resultImg[y,x2] != True:\n",
    "            cv2.floodFill(resultImg, high_region_mask, (x2, y),True, cv2.FLOODFILL_MASK_ONLY)\n",
    "            high_list.append((x2,y))\n",
    "            \n",
    "    for x in range(plane_edge, x2):\n",
    "        if high_region_mask[y1+1,x+1] == 0 and resultImg[y1,x] != True:\n",
    "            cv2.floodFill(resultImg, high_region_mask, (x, y1),True, cv2.FLOODFILL_MASK_ONLY)\n",
    "            high_list.append((x, y1))\n",
    "            \n",
    "        if high_region_mask[y2+1,x+1] == 0 and resultImg[y2,x] != True:\n",
    "            cv2.floodFill(resultImg, high_region_mask, (x, y2),True, cv2.FLOODFILL_MASK_ONLY)\n",
    "            high_list.append((x, y2))\n",
    "    \n",
    "    return resultImg, high_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Hand_mask(Edge_map, high_list, High_region_Image):\n",
    "    \"\"\"\n",
    "    Flood fill from high region position and stop when reach edge\n",
    "    Only fill in the hight region mask \n",
    "    ==> Prevent flood fill from seed which filled region does not the hight region\n",
    "    \"\"\"\n",
    "    h, w = Edge_map.shape[:2]\n",
    "    \n",
    "    resultImg = Edge_map.copy()\n",
    "    resultImg.dtype = 'uint8'\n",
    "    mask = np.zeros((h+2,w+2), np.uint8)\n",
    "    mask1 = np.ones((h+2,w+2), np.uint8)\n",
    "    mask1[1:h+1,1:w+1] = High_region_Image == False\n",
    "\n",
    "    for i in range(len(high_list)):\n",
    "        cv2.floodFill(resultImg, mask1, high_list[i],True,cv2.FLOODFILL_FIXED_RANGE)\n",
    "    \n",
    "    resultImg = resultImg - Edge_map\n",
    "    \n",
    "    return resultImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Fingertip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fingertip(hand_mask, debug = False):\n",
    "    '''\n",
    "        return: \n",
    "        @cnt: contours\n",
    "    '''\n",
    "    fingertips = []\n",
    "    low_threshold = 0\n",
    "    high_threshold = 1\n",
    "    kernel_size = 7\n",
    "    \n",
    "    #Smooth the mask and find the contours\n",
    "    smooth_mask = cv2.GaussianBlur(hand_mask,(kernel_size, kernel_size), 0)\n",
    "    (_, cnts, _) = cv2.findContours(smooth_mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    #convert the result image into RGB image\n",
    "    contours_image = cv2.convertScaleAbs(smooth_mask, alpha=(255))\n",
    "    contours_image = cv2.cvtColor(contours_image, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Find the contour with max area\n",
    "    maxArea = 0\n",
    "    hull = None\n",
    "    for i in range(len(cnts)):\n",
    "        area = cv2.contourArea(cnts[i])\n",
    "        if area > maxArea:\n",
    "            maxArea = area\n",
    "            hull = cv2.convexHull(cnts[i])\n",
    "            Contours = cnts[i]\n",
    "    \n",
    "    cx, cy = (0,0)\n",
    "    if hull is not None : \n",
    "        # get centroid from hand, then Draw the center and Hull_Convex\n",
    "        M = cv2.moments(hull)\n",
    "        cx = int(M[\"m10\"]/M[\"m00\"])\n",
    "        cy = int(M[\"m01\"]/M[\"m00\"])\n",
    "        if(debug == True):\n",
    "            cv2.circle(contours_image, (cx, cy), 5, (0,0,255), 3)\n",
    "        cv2.drawContours(contours_image, [Contours], -1, (255,0,0), 2)\n",
    "        \n",
    "        #Find the Fingertips\n",
    "        fingertips, contours_image = find_fingertips_by_dot(Contours, hull, contours_image)\n",
    "\n",
    "    return cnts, contours_image, (cx, cy), np.asarray(fingertips, dtype=np.int32)\n",
    "\n",
    "\n",
    "def find_fingertips_by_dot(Contours, hull, contours_image, debug=False):\n",
    "    fingertips = []\n",
    "    skip = 2\n",
    "    Convex = (0,0)\n",
    "    threshold = 10\n",
    "    first = True\n",
    "    for i in range(skip, len(Contours)-skip):\n",
    "        p = Contours[i-skip]\n",
    "        q = Contours[i]\n",
    "        r = Contours[i+skip]\n",
    "\n",
    "        dot = np.dot(q-p,(r-p).T)\n",
    "        if abs(dot) < 20:\n",
    "            points = (p[0,0],p[0,1])\n",
    "            if first == True:\n",
    "                Convex = points\n",
    "                first = False\n",
    "            IsnotEdge = points[0]!=0 and points[1]!=0 and points[0]!=contours_image.shape[1] and points[1]!=contours_image.shape[0]\n",
    "            IsConvex = Points_is_convex(hull, points, threshold)\n",
    "            Labeled = abs(Convex[0] - points[0]) > threshold and abs(Convex[1] - points[1]) > threshold\n",
    "            #if the point is in the convex list and haven't be labeled, then add it to Fingertip list\n",
    "            if IsConvex and Labeled and IsnotEdge:\n",
    "                Convex = points\n",
    "                fingertips.append(Convex)\n",
    "                if(debug==True):\n",
    "                    cv2.circle(contours_image, Convex, 5 , (0,255,0) , 3)\n",
    "    return fingertips, contours_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cotour Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Points_is_convex_all(hull, points, threshold):\n",
    "    label = False\n",
    "    label_hull = hull.copy()\n",
    "    for i in range(len(hull)):\n",
    "        if abs(hull[i,0,0] - points[0]) <= threshold and abs(hull[i,0,1] - points[1]) <= threshold:\n",
    "            label_hull[i,0] = (-1, -1)\n",
    "            label = True\n",
    "    return label, label_hull\n",
    "\n",
    "def Points_is_convex(hull, points, threshold):\n",
    "    label = False\n",
    "    label_hull = hull.copy()\n",
    "    for i in range(len(hull)):\n",
    "        if abs(hull[i,0,0] - points[0]) <= threshold and abs(hull[i,0,1] - points[1]) <= threshold:\n",
    "            label_hull[i,0] = (-1, -1)\n",
    "            label = True\n",
    "            return label\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_open_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##True: white 1  False: black 0\n",
    "# ptset = np.array(([False, False, False],\n",
    "#             [True, True, False],\n",
    "#             [False, True, False]))\n",
    "# plt.imshow(ptset,plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: 456.rrf\n",
      "====================================\n",
      "        Camera information\n",
      "====================================\n",
      "Type:            PICOFLEXX\n",
      "Width:           224\n",
      "Height:          171\n",
      "Operation modes: 1\n",
      "    MODE_PLAYBACK\n",
      "        this operation mode has 199753984 streams\n",
      "CameraInfo items: 0\n",
      "isConnected True\n",
      "getFrameRate 0\n",
      "200\n",
      "Inliner Number\n",
      " 35028\n",
      "Inliner plane\n",
      " [ 0.00004043  0.0000803  -0.00316866  0.00065545]\n",
      "firstly RANSAM\n",
      "depth: 0.0031\n",
      "*get threshold edge: 0.9055 s\n",
      "get edge map: 0.9094 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0000 s\n",
      "find fingertip: 0.0020 s\n",
      "hand tracking: 0.0000 s\n",
      "touched detection: 0.0000 s\n",
      "depth: 0.0030\n",
      "*get threshold edge: 0.9275 s\n",
      "get edge map: 0.9295 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0000 s\n",
      "find fingertip: 0.0010 s\n",
      "hand tracking: 0.0010 s\n",
      "touched detection: 0.0010 s\n",
      "depth: 0.0020\n",
      "*get threshold edge: 0.8805 s\n",
      "get edge map: 0.8815 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0010 s\n",
      "hand tracking: 0.0000 s\n",
      "touched detection: 0.0000 s\n",
      "depth: 0.0050\n",
      "*get threshold edge: 0.8855 s\n",
      "get edge map: 0.8875 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0040 s\n",
      "hand tracking: 0.0010 s\n",
      "touched detection: 0.0000 s\n",
      "depth: 0.0050\n",
      "*get threshold edge: 0.8675 s\n",
      "get edge map: 0.8695 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0015 s\n",
      "hand tracking: 0.0000 s\n",
      "touched detection: 0.0010 s\n",
      "depth: 0.0050\n",
      "*get threshold edge: 0.9039 s\n",
      "get edge map: 0.9059 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0020 s\n",
      "hand tracking: 0.0000 s\n",
      "touched detection: 0.0000 s\n",
      "depth: 0.0050\n",
      "*get threshold edge: 1.0284 s\n",
      "get edge map: 1.0314 s\n",
      "get hight region: 0.0010 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0110 s\n",
      "hand tracking: 0.0020 s\n",
      "touched detection: 0.0000 s\n",
      "depth: 0.0040\n",
      "*get threshold edge: 1.1433 s\n",
      "get edge map: 1.1453 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0030 s\n",
      "hand tracking: 0.0000 s\n",
      "touched detection: 0.0000 s\n",
      "depth: 0.0040\n",
      "*get threshold edge: 0.8995 s\n",
      "get edge map: 0.9005 s\n",
      "get hight region: 0.0010 s\n",
      "get hand mask: 0.0000 s\n",
      "find fingertip: 0.0010 s\n",
      "hand tracking: 0.0010 s\n",
      "touched detection: 0.0000 s\n",
      "depth: 0.0040\n",
      "*get threshold edge: 0.8735 s\n",
      "get edge map: 0.8745 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0080 s\n",
      "hand tracking: 0.0010 s\n",
      "touched detection: 0.0010 s\n",
      "depth: 0.0040\n",
      "*get threshold edge: 0.9065 s\n",
      "get edge map: 0.9085 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0080 s\n",
      "hand tracking: 0.0010 s\n",
      "touched pos: (50, 144)\n",
      "touched pos: (103, 165)\n",
      "touched detection: 0.0010 s\n",
      "depth: 0.0050\n",
      "*get threshold edge: 0.8545 s\n",
      "get edge map: 0.8565 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0070 s\n",
      "hand tracking: 0.0010 s\n",
      "touched detection: 0.0010 s\n",
      "depth: 0.0020\n",
      "*get threshold edge: 0.9875 s\n",
      "get edge map: 0.9904 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0000 s\n",
      "find fingertip: 0.0040 s\n",
      "hand tracking: 0.0010 s\n",
      "touched detection: 0.0000 s\n",
      "depth: 0.0050\n",
      "*get threshold edge: 0.8785 s\n",
      "get edge map: 0.8805 s\n",
      "get hight region: 0.0020 s\n",
      "get hand mask: 0.0010 s\n",
      "find fingertip: 0.0060 s\n",
      "hand tracking: 0.0010 s\n",
      "touched detection: 0.0010 s\n"
     ]
    }
   ],
   "source": [
    "def main ():\n",
    "    parser = argparse.ArgumentParser(usage=__doc__)\n",
    "    add_camera_opener_options(parser)\n",
    "    parser.add_argument(\"--seconds\", type=int, default=15, help=\"duration to capture data\")\n",
    "    options = parser.parse_args(args=['--rrf','456.rrf','--seconds', '5'])\n",
    "#     options = parser.parse_args(args=['--seconds', '5'])\n",
    "    opener = CameraOpener(options)\n",
    "    cam = opener.open_camera()\n",
    "\n",
    "    print_camera_info (cam)\n",
    "    print(\"isConnected\", cam.isConnected())\n",
    "    print(\"getFrameRate\", cam.getFrameRate())\n",
    "    print(cam.getFilterLevel())\n",
    "\n",
    "    z_queue = queue.Queue()\n",
    "    gray_queue = queue.Queue()\n",
    "    points3D_queue = queue.Queue()\n",
    "    ConfidenceIndex_queue = queue.Queue()\n",
    "    Confidence_queue = queue.Queue()\n",
    "    l = MyListener(z_queue,gray_queue,points3D_queue,ConfidenceIndex_queue,Confidence_queue)\n",
    "\n",
    "    cam.registerDataListener(l)\n",
    "    cam.startCapture()\n",
    "\n",
    "    process_event(z_queue, gray_queue, points3D_queue, ConfidenceIndex_queue, Confidence_queue, painter=l, seconds=150)\n",
    "\n",
    "    cam.stopCapture()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "if (__name__ == \"__main__\"):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAD8CAYAAAAVOD3kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA3dJREFUeJzt28FpAzEURVGNmSpchZswrsBVpoKQJlyFy7BSQLhgCEQTOGetxVtd/kbbnHMA8NNp9QCAoxJIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEDYVw8YY4zr6e47D/Bnvl4f2zvvXJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRD21QOO7PP5WD0B/rXb+bJ6wq+4IAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiDsqwcc2e18WT0BWMgFCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAYZtzrt4AcEguSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiB8A6e4D4jy7aDrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show plane edge\n",
    "test = np.zeros((171,224), np.uint8)\n",
    "l = 30\n",
    "test[l:171-l,l:224-l] = 1\n",
    "plt.imshow(test),plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[  0.   1.   2.   3.   4.]\n",
      " [  5.   6.   7.   8.   9.]\n",
      " [ 10.  11.  12.  13.  14.]\n",
      " [ 15.  16.  17.  18.  19.]\n",
      " [ 20.  21.  22.  23.  24.]]\n",
      "[[  6.   7.   8.]\n",
      " [ 11.  12.  13.]\n",
      " [ 16.  17.  18.]]\n",
      "9\n",
      "[[  6.]\n",
      " [  7.]\n",
      " [  8.]\n",
      " [ 11.]\n",
      " [ 12.]\n",
      " [ 13.]\n",
      " [ 16.]\n",
      " [ 17.]\n",
      " [ 18.]]\n",
      "0 [ 6.]\n",
      "1 [ 7.]\n",
      "2 [ 8.]\n",
      "3 [ 11.]\n",
      "4 [ 12.]\n",
      "5 [ 13.]\n",
      "6 [ 16.]\n",
      "7 [ 17.]\n",
      "8 [ 18.]\n"
     ]
    }
   ],
   "source": [
    "u, v = (2, 2)\n",
    "kernal = 3\n",
    "x = np.zeros((5, 5))\n",
    "index = 0\n",
    "for h in range(0, 5):\n",
    "    for w in range(0, 5):\n",
    "        x[h, w] = index\n",
    "        index = index + 1\n",
    "bounding_sixe = math.floor(kernal/2)\n",
    "print(bounding_sixe)\n",
    "y = x[(u-bounding_sixe):(u+bounding_sixe+1), (v-bounding_sixe):(v+bounding_sixe+1)]\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "print(y.size)\n",
    "\n",
    "y = np.reshape(y, (y.size,-1))\n",
    "print(y)\n",
    "for i, test in enumerate(y):\n",
    "    print(i, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
